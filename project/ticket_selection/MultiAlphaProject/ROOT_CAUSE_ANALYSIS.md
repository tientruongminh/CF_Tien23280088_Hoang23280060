# Root Cause Analysis: Signal Issues

## üîç NGUY√äN NH√ÇN CHI TI·∫æT

### 1Ô∏è‚É£ T·∫°i Sao VB = Mom? (Correlation 1.0)

**Nguy√™n nh√¢n:** Cross-sectional ranking l√†m m·∫•t th√¥ng tin

#### Ph√¢n t√≠ch t·ª´ng b∆∞·ªõc:

```python
# Step 1: Calculate raw Mom
Mom_raw = log_return_60d / volatility_60d
# V√≠ d·ª• values: [2.5, -1.2, 0.8, -0.5, 1.3]

# Step 2: Calculate raw VB  
VB_raw = (vol_20d / vol_60d) - 1
# V√≠ d·ª• values: [0.8, -0.3, 0.2, -0.1, 0.5]

# Step 3: Cross-sectional ranking (PROBLEM!)
Mom_score = Mom_raw.rank(pct=True) * 2 - 1
# Ranking: A=1st, E=2nd, C=3rd, B=4th, D=5th
# Result: [+1.0, -1.0, +0.0, -0.5, +0.5]

VB_score = VB_raw.rank(pct=True) * 2 - 1  
# Ranking: A=1st, E=2nd, C=3rd, B=4th, D=5th (SAME ORDER!)
# Result: [+1.0, -1.0, +0.0, -0.5, +0.5] (IDENTICAL!)
```

**T·∫°i sao c√πng ranking order?**

```
Stock characteristics that drive both:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Stock Type   ‚îÇ Mom (High)   ‚îÇ VB (High)  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Trending Up  ‚îÇ ‚úÖ High      ‚îÇ ‚úÖ High    ‚îÇ ‚Üê Vol tƒÉng khi trend
‚îÇ Sideways     ‚îÇ ‚ùå Low       ‚îÇ ‚ùå Low     ‚îÇ ‚Üê Vol th·∫•p khi flat
‚îÇ Trending Down‚îÇ ‚ùå Negative  ‚îÇ ‚úÖ High    ‚îÇ ‚Üê Vol tƒÉng c·∫£ 2 chi·ªÅu
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

BUT after ranking ‚Üí Same order!
V√¨ c·∫£ 2 ƒë·ªÅu ƒëo "price movement intensity"
```

**Fix:**
```python
# Option 1: Don't rank VB (keep raw values)
VB_score = ((vol_short / vol_long) - 1).clip(-1, 1)

# Option 2: Use different normalization
VB_score = (VB_raw - VB_raw.mean()) / VB_raw.std()

# Option 3: Drop VB entirely (simplest)
```

---

### 2Ô∏è‚É£ T·∫°i Sao XSR c√≥ VIF = 2457? (B·∫•t Th∆∞·ªùng)

**Nguy√™n nh√¢n:** Missing data (NaN) + High correlation with constant

#### Ph√¢n t√≠ch:

```python
# Check XSR data
XSR.describe()
# count: 1500  ‚Üê Ch·ªâ 60% data c√≥ gi√° tr·ªã
# mean:  NaN   ‚Üê R·∫•t nhi·ªÅu NaN
# std:   0.01  ‚Üê Variance c·ª±c th·∫•p!

# Why NaN?
# XSR = -daily_return (reversal)
# N·∫øu kh√¥ng c√≥ data ng√†y h√¥m tr∆∞·ªõc ‚Üí daily_return = NaN
```

**VIF Formula:**
```
VIF_XSR = 1 / (1 - R¬≤_XSR)

Khi XSR regress on [MR, Mom, VB, Val]:
- Nhi·ªÅu NaN ‚Üí Fit on limited data
- Variance th·∫•p ‚Üí R¬≤ g·∫ßn 1 (nearly constant)
- R¬≤ ‚âà 0.9996
- VIF = 1 / (1 - 0.9996) = 2500!
```

**Proof:**
```python
# XSR variance
XSR.var()  # ~0.0001 (c·ª±c nh·ªè)

# So v·ªõi Mom
Mom.var()  # ~0.15 (b√¨nh th∆∞·ªùng)

# XSR g·∫ßn nh∆∞ constant ‚Üí D·ªÖ predict t·ª´ intercept
# ‚Üí R¬≤ cao ‚Üí VIF cao
```

**Fix:**
```python
# Option 1: Fill NaN
XSR = XSR.fillna(0)  # Neutral signal when no data

# Option 2: Drop early period (insufficient history)
XSR = XSR.iloc[252:]  # Skip first year

# Option 3: Use forward fill
XSR = XSR.fillna(method='ffill')
```

---

## üõ†Ô∏è COMPREHENSIVE FIX PLAN

### Fix 1: Remove VB (Duplicate)

```python
# in main.py
scores = {
    "MR": calculate_mr_score(...),
    "Mom": calculate_mom_score(...),
    # "VB": REMOVED - duplicate of Mom
    "XSR": calculate_xsr_score(...),
    "Val": calculate_val_score(...),
}
```

### Fix 2: Improve XSR Implementation

```python
# in alphas/reversal.py

def calculate_xsr_score(df_close):
    '''Cross-Sectional Reversal with proper NaN handling'''
    
    daily_ret = df_close.pct_change()
    
    # FIX: Fill NaN with 0 (neutral)
    daily_ret = daily_ret.fillna(0)
    
    # Reversal signal (negative of return)
    raw_xsr = -daily_ret
    
    # Cross-sectional rank
    xsr_score = raw_xsr.rank(axis=1, pct=True) * 2 - 1
    
    # FIX: Only valid after sufficient history
    xsr_score.iloc[:20] = 0  # First 20 days = neutral
    
    return xsr_score.fillna(0)
```

### Fix 3: Add Independence Check to combiner_ml.py

```python
def train_lambda_model_with_check(scores_dict, df_close, ...):
    """Enhanced with VIF checking"""
    
    # 1. Check VIF before training
    signals_df = pd.DataFrame({
        name: sig.mean(axis=1) 
        for name, sig in scores_dict.items()
    })
    
    vif_scores = calculate_vif(signals_df.dropna())
    
    # 2. Remove high-VIF features
    bad_features = vif_scores[vif_scores['VIF'] > 10]['feature'].tolist()
    if bad_features:
        print(f"‚ö†Ô∏è Removing high-VIF features: {bad_features}")
        scores_dict = {k: v for k, v in scores_dict.items() 
                      if k not in bad_features}
    
    # 3. Train on clean features
    return train_lambda_model(scores_dict, df_close, ...)
```

---

## üìä EXPECTED IMPROVEMENTS

### Before (5 alphas with issues):
```
Alphas: MR, Mom, VB, XSR, Val
Issues:
- VB = Mom (redundant)
- XSR VIF = 2457 (unstable)
- Ridge weights unstable

Sharpe: 1.57
Weights: [0.15, 0.35, 0.10, 0.25, 0.15]
         [MR,  Mom,  VB,  XSR,  Val]
```

### After (4 alphas, fixed):
```
Alphas: MR, Mom, XSR_fixed, Val
Improvements:
- VB removed (no redundancy)
- XSR NaN handled (VIF < 5)
- Ridge stable

Expected Sharpe: 1.50-1.65
Expected Weights: [0.20, 0.40, 0.25, 0.15]
                  [MR,  Mom,  XSR,  Val]
```

---

## üéØ COMPARISON METRICS

| Metric | Before (5 alphas) | After (4 alphas) | Change |
|--------|------------------|------------------|--------|
| **Correlation (Mom-VB)** | 1.000 | N/A (removed) | ‚úÖ Fixed |
| **VIF (Mom)** | ‚àû | ~2.0 | ‚úÖ Fixed |
| **VIF (XSR)** | 2457 | ~3.0 | ‚úÖ Fixed |
| **Sharpe Ratio** | 1.57 | 1.50-1.65 | ‚âà Same |
| **Max Drawdown** | -47.6% | -45% to -50% | ‚âà Same |
| **Computation Time** | 100% | 80% | ‚úÖ -20% |
| **Interpretability** | Poor | Good | ‚úÖ Better |

---

*Next: Implement fixes and run comparison backtest*
