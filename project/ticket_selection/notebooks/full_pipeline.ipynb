{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":13985133,"datasetId":8681142,"databundleVersionId":14760339}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nFull pair trading cluster pipeline trên toàn universe:\n\n1) Lấy danh sách ticker từ folder per_symbol (price volume).\n2) Lấy sectorKey, industryKey bằng yahooquery (hoặc đọc từ sector_industry.csv nếu đã có).\n3) Tính volatility 1 năm cho toàn bộ universe, chia decile.\n4) Tính beta với SPY cho toàn bộ universe.\n5) Với từng cặp (sectorKey, industryKey) có >= MIN_GROUP_SIZE:\n   - Lọc theo volatility decile (mid vol).\n   - Lọc theo beta gần nhau (median ± BETA_TOL).\n   - Tính return 3 năm, average dollar volume và chọn dv band.\n   - Chọn top tickers theo mean pairwise correlation trong band.\n   - Test cointegration tất cả cặp trong cụm con, lấy các mã có pvalue < COINT_ALPHA.\n   - Xuất file csv cuối cùng với OHLCV sạch cho các mã cointegrated.\n\nOutput: một folder OUTPUT_DIR chứa các file:\n  cluster_{sectorKey}_{industryKey}.csv\n\"\"\"\n\nimport os\nfrom itertools import combinations\nfrom functools import reduce\n\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.tsa.stattools import coint\n\n# Optional: chỉ cần khi cần fetch sector/industry lần đầu\n# !pip install -q yahooquery\ntry:\n    from yahooquery import Ticker as YQTicker\nexcept ImportError:\n    YQTicker = None\n\n# ========================\n# CONFIG\n# ========================\n\n# Thư mục chứa các file price volume per symbol\nDATA_DIR = \"/kaggle/input/computational-finance/yfinance/yfinance/per_symbol\"\n\n# File SPY riêng\nSPY_PATH = \"/kaggle/input/computational-finance/SPY.csv\"\n\n# File sector industry (nếu đã tạo trước)\nSECTOR_FILE = \"/kaggle/input/computational-finance/sector_industry.csv\"\n\n# Folder output cuối cùng\nOUTPUT_DIR = \"clusters\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Ngưỡng và tham số\nMIN_GROUP_SIZE = 50          # số mã tối thiểu cho một cặp sector industry\nVOL_MIN_OBS = 200             # số quan sát tối thiểu để tính vol 1y\nVOL_LOOKBACK_DAYS = 252       # số ngày gần nhất để tính vol 1y\nVOL_DECILES_KEEP = [4, 5, 6]  # dải volatility giữ lại\n\nBETA_MIN_OBS = 200\nBETA_TOL = 0.2                # beta gần median ± 0.2\n\nRET_LOOKBACK_YEARS = 3        # số năm lookback cho return, corr, dollar vol\nRET_MIN_OBS = 200\n\nDV_BAND_LOW = 0.5             # chọn mã có avg dollar volume trong [0.5, 2] * median\nDV_BAND_HIGH = 2.0\n\nTOP_K_BY_CORR = 20            # chọn tối đa 10 mã có mean corr cao nhất trong band dv\n\nCOINT_LOOKBACK_YEARS = 3\nCOINT_MIN_OBS = 200\nCOINT_ALPHA = 0.05            # pvalue < 0.05 thì coi là cointegrated\n\n\n# ========================\n# STEP 0: UTILITIES\n# ========================\n\ndef list_tickers(data_dir=DATA_DIR):\n    return sorted(\n        [name.split(\".\")[0] for name in os.listdir(data_dir) if name.endswith(\".csv\")]\n    )\n\n\ndef load_ohlcv(ticker, data_dir=DATA_DIR):\n    \"\"\"\n    Đọc file 1 ticker, trả về DataFrame với cột:\n      Date (datetime), Open, High, Low, Close, Volume (numeric)\n    Tự xử lý một số kiểu header noise (dòng đầu chứa ticker, v.v.).\n    \"\"\"\n    path = os.path.join(data_dir, f\"{ticker}.csv\")\n    if not os.path.exists(path):\n        print(f\"[warn] missing file: {path}\")\n        return None\n\n    try:\n        df = pd.read_csv(path)\n    except Exception as e:\n        print(f\"[warn] read error {ticker}: {e}\")\n        return None\n\n    # Cột ngày\n    if \"Date\" in df.columns:\n        date_col = \"Date\"\n    elif \"date\" in df.columns:\n        date_col = \"date\"\n    else:\n        # fallback kiểu file SPY dạng Price + dòng Date (ít gặp với per_symbol)\n        if \"Price\" in df.columns and str(df.loc[1, \"Price\"]).lower() == \"date\":\n            df = df.iloc[2:].copy()\n            df = df.rename(columns={\"Price\": \"Date\"})\n            date_col = \"Date\"\n        else:\n            print(f\"[warn] no date col {ticker}\")\n            return None\n\n    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n    df = df.dropna(subset=[date_col])\n    if df.empty:\n        print(f\"[warn] empty after date parse {ticker}\")\n        return None\n\n    df = df.sort_values(date_col)\n\n    # Map cột\n    col_map = {\"Open\": None, \"High\": None, \"Low\": None, \"Close\": None, \"Volume\": None}\n\n    for col in [\"Open\", \"open\"]:\n        if col in df.columns:\n            col_map[\"Open\"] = col\n            break\n    for col in [\"High\", \"high\"]:\n        if col in df.columns:\n            col_map[\"High\"] = col\n            break\n    for col in [\"Low\", \"low\"]:\n        if col in df.columns:\n            col_map[\"Low\"] = col\n            break\n    for col in [\"Adj Close\", \"Adj_Close\", \"adj_close\", \"Close\", \"close\"]:\n        if col in df.columns:\n            col_map[\"Close\"] = col\n            break\n    for col in [\"Volume\", \"volume\", \"Vol\", \"vol\"]:\n        if col in df.columns:\n            col_map[\"Volume\"] = col\n            break\n\n    if col_map[\"Close\"] is None or col_map[\"Volume\"] is None:\n        print(f\"[warn] missing Close or Volume for {ticker}\")\n        return None\n\n    # Convert numeric\n    for key, col in col_map.items():\n        if col is not None:\n            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n\n    df = df.dropna(subset=[col_map[\"Close\"], col_map[\"Volume\"]])\n    if df.empty:\n        print(f\"[warn] empty after numeric {ticker}\")\n        return None\n\n    out = pd.DataFrame(\n        {\n            \"Date\": df[date_col],\n            \"Open\": df[col_map[\"Open\"]] if col_map[\"Open\"] is not None else np.nan,\n            \"High\": df[col_map[\"High\"]] if col_map[\"High\"] is not None else np.nan,\n            \"Low\": df[col_map[\"Low\"]] if col_map[\"Low\"] is not None else np.nan,\n            \"Close\": df[col_map[\"Close\"]],\n            \"Volume\": df[col_map[\"Volume\"]],\n        }\n    )\n\n    return out\n\n\n# ========================\n# STEP 1: SECTOR INDUSTRY\n# ========================\n\ndef get_sector_industry(tickers, sector_file=SECTOR_FILE):\n    \"\"\"\n    Lấy bảng sectorKey, industryKey cho toàn bộ tickers.\n    Nếu đã có sector_file thì đọc, nếu chưa có và YQTicker khả dụng thì fetch từ yahooquery.\n    \"\"\"\n    if os.path.exists(sector_file):\n        df_sector = pd.read_csv(sector_file)\n        return df_sector\n\n    if YQTicker is None:\n        raise RuntimeError(\n            \"sector_industry.csv không tồn tại và yahooquery không khả dụng. \"\n            \"Cài đặt yahooquery hoặc tạo sector_industry.csv trước.\"\n        )\n\n    t = YQTicker(tickers)\n    profiles = t.asset_profile\n\n    rows = []\n    for symbol, info in profiles.items():\n        if not isinstance(info, dict):\n            continue\n        rows.append(\n            {\n                \"ticker\": symbol,\n                \"sectorKey\": info.get(\"sectorKey\"),\n                \"industryKey\": info.get(\"industryKey\"),\n            }\n        )\n\n    df_sector = pd.DataFrame(rows)\n    df_sector = df_sector[~df_sector[\"sectorKey\"].isna()]\n    df_sector.to_csv(sector_file, index=False)\n    return df_sector\n\n\n# ========================\n# STEP 2: VOLATILITY 1Y\n# ========================\n\ndef compute_1y_vol(ticker, data_dir=DATA_DIR, min_obs=VOL_MIN_OBS):\n    \"\"\"\n    Tính annualized volatility 1 năm gần nhất trên log return Close.\n    Dùng tối đa VOL_LOOKBACK_DAYS phiên gần nhất.\n    \"\"\"\n    df = load_ohlcv(ticker, data_dir=data_dir)\n    if df is None:\n        return None\n\n    df_recent = df.sort_values(\"Date\").tail(VOL_LOOKBACK_DAYS)\n    prices = df_recent[\"Close\"].astype(float)\n    rets = np.log(prices).diff().dropna()\n\n    if rets.shape[0] < min_obs:\n        return None\n\n    vol_1y = rets.std() * np.sqrt(252.0)\n    return {\"ticker\": ticker, \"vol_1y\": vol_1y}\n\n\ndef compute_all_vols(tickers, data_dir=DATA_DIR):\n    rows = []\n    for tk in tickers:\n        res = compute_1y_vol(tk, data_dir=data_dir)\n        if res is not None:\n            rows.append(res)\n    df_vol = pd.DataFrame(rows)\n    df_vol = df_vol.replace([np.inf, -np.inf], np.nan).dropna(subset=[\"vol_1y\"])\n    df_vol = df_vol[df_vol[\"vol_1y\"] > 0]\n    df_vol[\"vol_decile\"] = pd.qcut(df_vol[\"vol_1y\"], 10, labels=False) + 1\n    return df_vol\n\n\n# ========================\n# STEP 3: BETA VS SPY\n# ========================\n\ndef load_spy(spy_path=SPY_PATH):\n    \"\"\"\n    Load SPY với các format tương tự file riêng của bạn:\n    cột Price, Close, High, Low, Open, Volume với hai dòng đầu là header noise.\n    \"\"\"\n    df = pd.read_csv(spy_path)\n\n    if \"Date\" in df.columns or \"date\" in df.columns:\n        date_col = \"Date\" if \"Date\" in df.columns else \"date\"\n    elif \"Price\" in df.columns and str(df.loc[1, \"Price\"]).lower() == \"date\":\n        df = df.iloc[2:].copy()\n        df = df.rename(columns={\"Price\": \"Date\"})\n        date_col = \"Date\"\n    else:\n        return None\n\n    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n    df = df.dropna(subset=[date_col])\n    df = df.sort_values(date_col)\n\n    # Chọn cột giá\n    price_col = None\n    for col in [\"Adj Close\", \"Adj_Close\", \"adj_close\", \"Close\", \"close\"]:\n        if col in df.columns:\n            price_col = col\n            break\n    if price_col is None:\n        return None\n\n    df[price_col] = pd.to_numeric(df[price_col], errors=\"coerce\")\n    df = df.dropna(subset=[price_col])\n    if df.empty:\n        return None\n\n    df = df[[date_col, price_col]].rename(columns={date_col: \"Date\", price_col: \"Close\"})\n    return df\n\n\ndef compute_spy_returns(spy_path=SPY_PATH, lookback_days=VOL_LOOKBACK_DAYS):\n    df_spy = load_spy(spy_path)\n    if df_spy is None or df_spy.empty:\n        raise RuntimeError(\"Không load được SPY\")\n\n    df_spy = df_spy.sort_values(\"Date\").tail(lookback_days).copy()\n    df_spy[\"r_m\"] = np.log(df_spy[\"Close\"]).diff()\n    spy_ret = df_spy[[\"Date\", \"r_m\"]].dropna().reset_index(drop=True)\n    return spy_ret\n\n\ndef compute_beta_vs_spy(ticker, spy_ret, data_dir=DATA_DIR, min_obs=BETA_MIN_OBS):\n    df = load_ohlcv(ticker, data_dir=data_dir)\n    if df is None:\n        return None\n\n    start_date = spy_ret[\"Date\"].min()\n    end_date = spy_ret[\"Date\"].max()\n\n    df = df[(df[\"Date\"] >= start_date) & (df[\"Date\"] <= end_date)].copy()\n    df[\"r_i\"] = np.log(df[\"Close\"]).diff()\n    df_ret = df[[\"Date\", \"r_i\"]].dropna()\n\n    merged = pd.merge(df_ret, spy_ret, on=\"Date\", how=\"inner\")\n    if merged.shape[0] < min_obs:\n        return None\n\n    ri = merged[\"r_i\"].values\n    rm = merged[\"r_m\"].values\n\n    cov_im = np.cov(ri, rm, ddof=1)[0, 1]\n    var_m = np.var(rm, ddof=1)\n    if var_m == 0:\n        return None\n\n    beta = cov_im / var_m\n    return {\"ticker\": ticker, \"beta_spy\": beta}\n\n\ndef compute_all_betas(tickers, spy_ret, data_dir=DATA_DIR):\n    rows = []\n    for tk in tickers:\n        res = compute_beta_vs_spy(tk, spy_ret, data_dir=data_dir)\n        if res is not None:\n            rows.append(res)\n    df_beta = pd.DataFrame(rows)\n    return df_beta\n\n\n# ========================\n# STEP 4: RETURN MATRIX, DOLLAR VOLUME, CORRELATION\n# ========================\n\ndef build_price_dict(tickers, data_dir=DATA_DIR):\n    price_data = {}\n    for tk in tickers:\n        df = load_ohlcv(tk, data_dir=data_dir)\n        if df is not None:\n            price_data[tk] = df\n    return price_data\n\n\ndef build_common_return_matrix(price_data, lookback_years=RET_LOOKBACK_YEARS):\n    # Common date range\n    start_dates = [df[\"Date\"].min() for df in price_data.values()]\n    end_dates = [df[\"Date\"].max() for df in price_data.values()]\n    latest_start = max(start_dates)\n    earliest_end = min(end_dates)\n\n    three_years = pd.Timestamp(earliest_end) - pd.Timedelta(days=365 * lookback_years)\n    range_start = max(latest_start, three_years)\n    range_end = earliest_end\n\n    ret_list = []\n    for tk, df in price_data.items():\n        df_win = df[(df[\"Date\"] >= range_start) & (df[\"Date\"] <= range_end)].copy()\n        df_win[\"r\"] = np.log(df_win[\"Close\"]).diff()\n        df_ret = df_win[[\"Date\", \"r\"]].dropna()\n        df_ret = df_ret.rename(columns={\"r\": tk})\n        ret_list.append(df_ret)\n\n    if not ret_list:\n        return None, None, None\n\n    df_returns = reduce(\n        lambda left, right: pd.merge(left, right, on=\"Date\", how=\"inner\"),\n        ret_list,\n    )\n\n    df_returns = df_returns.sort_values(\"Date\").reset_index(drop=True)\n    return df_returns, range_start, range_end\n\n\ndef compute_avg_dollar_volume(price_data, range_start, range_end):\n    rows = []\n    for tk, df in price_data.items():\n        df_win = df[(df[\"Date\"] >= range_start) & (df[\"Date\"] <= range_end)].copy()\n        if df_win.empty:\n            continue\n        df_win[\"dollar_vol\"] = df_win[\"Close\"] * df_win[\"Volume\"]\n        avg_dv = df_win[\"dollar_vol\"].mean()\n        rows.append({\"ticker\": tk, \"avg_dollar_vol\": avg_dv})\n    df_dv = pd.DataFrame(rows).dropna(subset=[\"avg_dollar_vol\"])\n    return df_dv\n\n\n# ========================\n# STEP 5: COINTEGRATION\n# ========================\n\ndef find_cointegrated_pairs(df_cluster, tickers,\n                            lookback_years=COINT_LOOKBACK_YEARS,\n                            min_obs=COINT_MIN_OBS,\n                            alpha=COINT_ALPHA):\n    df_cluster = df_cluster.copy()\n    df_cluster[\"Date\"] = pd.to_datetime(df_cluster[\"Date\"])\n\n    max_date = df_cluster[\"Date\"].max()\n    start_cut = max_date - pd.Timedelta(days=365 * lookback_years)\n    df_win = df_cluster[df_cluster[\"Date\"] >= start_cut].copy()\n\n    results = []\n    good_pairs = []\n\n    for t1, t2 in combinations(tickers, 2):\n        s1 = df_win[df_win[\"ticker\"] == t1][[\"Date\", \"Close\"]]\n        s2 = df_win[df_win[\"ticker\"] == t2][[\"Date\", \"Close\"]]\n\n        merged = pd.merge(s1, s2, on=\"Date\", how=\"inner\", suffixes=(\"_1\", \"_2\"))\n        merged = merged.dropna()\n        if merged.shape[0] < min_obs:\n            continue\n\n        y1 = np.log(merged[\"Close_1\"].values)\n        y2 = np.log(merged[\"Close_2\"].values)\n\n        try:\n            stat, pvalue, crit = coint(y1, y2)\n        except Exception as e:\n            print(f\"[warn] coint error {t1}-{t2}: {e}\")\n            continue\n\n        results.append(\n            {\n                \"ticker1\": t1,\n                \"ticker2\": t2,\n                \"pvalue\": pvalue,\n                \"stat\": stat,\n            }\n        )\n\n        if pvalue < alpha:\n            good_pairs.append((t1, t2, pvalue))\n\n    res_df = pd.DataFrame(results).sort_values(\"pvalue\") if results else pd.DataFrame()\n    return res_df, good_pairs\n\n\ndef build_cluster_dataset(tickers, data_dir=DATA_DIR):\n    dfs = []\n    for tk in tickers:\n        df_tk = load_ohlcv(tk, data_dir=data_dir)\n        if df_tk is None:\n            continue\n        df_tk = df_tk.copy()\n        df_tk[\"ticker\"] = tk\n        dfs.append(df_tk)\n\n    if not dfs:\n        return None\n\n    df_all = pd.concat(dfs, ignore_index=True)\n    df_all = df_all.sort_values([\"Date\", \"ticker\"]).reset_index(drop=True)\n    return df_all[[\"Date\", \"ticker\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n\n\n# ========================\n# STEP 6: GROUP PIPELINE\n# ========================\n\ndef process_group(sector, industry, df_group, data_dir=DATA_DIR):\n    \"\"\"\n    Chạy pipeline cho một cặp (sectorKey, industryKey).\n    df_group: subset của universe với các cột ticker, vol_decile, beta_spy.\n    Trả về df_cluster_coint cuối cùng (hoặc None nếu fail).\n    \"\"\"\n    # 1. Lọc volatile mid decile\n    sub = df_group[df_group[\"vol_decile\"].isin(VOL_DECILES_KEEP)].copy()\n    if sub.shape[0] < 2:\n        return None\n\n    # 2. Lọc theo beta gần median\n    beta_center = sub[\"beta_spy\"].median()\n    mask_beta = sub[\"beta_spy\"].between(beta_center - BETA_TOL,\n                                        beta_center + BETA_TOL)\n    sub_beta = sub[mask_beta].copy()\n    if sub_beta.shape[0] < 2:\n        return None\n\n    tickers = sub_beta[\"ticker\"].unique().tolist()\n\n    # 3. Build price data và return matrix\n    price_data = build_price_dict(tickers, data_dir=data_dir)\n    if len(price_data) < 2:\n        return None\n\n    df_returns, range_start, range_end = build_common_return_matrix(\n        price_data,\n        lookback_years=RET_LOOKBACK_YEARS,\n    )\n    if df_returns is None or df_returns.shape[1] <= 2:\n        return None\n\n    # 4. Dollar volume band\n    df_dv = compute_avg_dollar_volume(price_data, range_start, range_end)\n    if df_dv.empty:\n        return None\n\n    median_dv = df_dv[\"avg_dollar_vol\"].median()\n    lower = median_dv * DV_BAND_LOW\n    upper = median_dv * DV_BAND_HIGH\n\n    dv_band = df_dv[\n        (df_dv[\"avg_dollar_vol\"] >= lower) & (df_dv[\"avg_dollar_vol\"] <= upper)\n    ]\n    band_tickers = set(dv_band[\"ticker\"].tolist())\n    if len(band_tickers) < 2:\n        return None\n\n    # 5. Correlation và chọn top tickers theo mean corr\n    ret_mat = df_returns.drop(columns=[\"Date\"])\n    valid_cols = [c for c in ret_mat.columns if c in band_tickers]\n    if len(valid_cols) < 2:\n        return None\n\n    corr_band = ret_mat[valid_cols].corr()\n\n    mean_corr_per_ticker = {}\n    for tk in valid_cols:\n        s = corr_band[tk].drop(tk)\n        mean_corr_per_ticker[tk] = s.mean()\n\n    top_sorted = sorted(\n        mean_corr_per_ticker,\n        key=mean_corr_per_ticker.get,\n        reverse=True,\n    )\n    k = min(TOP_K_BY_CORR, len(top_sorted))\n    cluster_tickers = top_sorted[:k]\n\n    # 6. Build cluster dataset và cointegration\n    df_cluster = build_cluster_dataset(cluster_tickers, data_dir=data_dir)\n    if df_cluster is None:\n        return None\n\n    res_df, good_pairs = find_cointegrated_pairs(\n        df_cluster,\n        cluster_tickers,\n        lookback_years=COINT_LOOKBACK_YEARS,\n        min_obs=COINT_MIN_OBS,\n        alpha=COINT_ALPHA,\n    )\n\n    if not good_pairs:\n        return None\n\n    cointegrated_tickers = sorted(\n        set([t1 for t1, t2, _ in good_pairs] + [t2 for t1, t2, _ in good_pairs])\n    )\n\n    df_cluster_coint = df_cluster[df_cluster[\"ticker\"].isin(cointegrated_tickers)].copy()\n    df_cluster_coint = (\n        df_cluster_coint.sort_values([\"Date\", \"ticker\"]).reset_index(drop=True)\n    )\n\n    return df_cluster_coint\n\n\n# ========================\n# STEP 7: MAIN PIPELINE\n# ========================\n\ndef run_full_pipeline():\n    # 1. Universe tickers\n    tickers = list_tickers(DATA_DIR)\n    print(\"Total tickers from per_symbol:\", len(tickers))\n\n    # 2. Sector industry\n    df_sector = get_sector_industry(tickers, sector_file=SECTOR_FILE)\n\n    # 3. Volatility 1y\n    print(\"Computing 1y volatility...\")\n    df_vol = compute_all_vols(tickers, data_dir=DATA_DIR)\n\n    # 4. Beta vs SPY\n    print(\"Computing beta vs SPY...\")\n    spy_ret = compute_spy_returns(SPY_PATH)\n    df_beta = compute_all_betas(tickers, spy_ret, data_dir=DATA_DIR)\n\n    # 5. Universe merged\n    universe = (\n        df_sector.merge(df_vol, on=\"ticker\", how=\"inner\")\n        .merge(df_beta, on=\"ticker\", how=\"inner\")\n    )\n\n    print(\"Universe after merge:\", universe.shape)\n\n    # 6. Group by sectorKey, industryKey\n    grouped = universe.groupby([\"sectorKey\", \"industryKey\"])\n\n    for (sector, industry), df_group in grouped:\n        if df_group.shape[0] < MIN_GROUP_SIZE:\n            continue\n\n        print(\n            f\"\\nProcessing group: sector={sector}, industry={industry}, \"\n            f\"n={df_group.shape[0]}\"\n        )\n\n        df_cluster_coint = process_group(sector, industry, df_group, data_dir=DATA_DIR)\n\n        if df_cluster_coint is None or df_cluster_coint.empty:\n            print(\"  No final cluster for this group.\")\n            continue\n\n        fname = f\"cluster_{sector}_{industry}.csv\"\n        fname = fname.replace(\" \", \"_\")\n        out_path = os.path.join(OUTPUT_DIR, fname)\n        df_cluster_coint.to_csv(out_path, index=False)\n        print(\n            f\"  Saved final cluster for {sector}/{industry} \"\n            f\"with {df_cluster_coint['ticker'].nunique()} tickers \"\n            f\"to {out_path}\"\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T07:13:47.190899Z","iopub.execute_input":"2025-12-04T07:13:47.192071Z","iopub.status.idle":"2025-12-04T07:13:47.259019Z","shell.execute_reply.started":"2025-12-04T07:13:47.192039Z","shell.execute_reply":"2025-12-04T07:13:47.257843Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"run_full_pipeline()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T07:13:51.517130Z","iopub.execute_input":"2025-12-04T07:13:51.517510Z","iopub.status.idle":"2025-12-04T07:15:33.952863Z","shell.execute_reply.started":"2025-12-04T07:13:51.517484Z","shell.execute_reply":"2025-12-04T07:15:33.951618Z"}},"outputs":[{"name":"stdout","text":"Total tickers from per_symbol: 1808\nComputing 1y volatility...\nComputing beta vs SPY...\nUniverse after merge: (1576, 7)\n\nProcessing group: sector=financial-services, industry=banks-regional, n=197\n  Saved final cluster for financial-services/banks-regional with 11 tickers to clusters/cluster_financial-services_banks-regional.csv\n\nProcessing group: sector=healthcare, industry=biotechnology, n=214\n  Saved final cluster for healthcare/biotechnology with 2 tickers to clusters/cluster_healthcare_biotechnology.csv\n\nProcessing group: sector=technology, industry=software-application, n=61\n  Saved final cluster for technology/software-application with 2 tickers to clusters/cluster_technology_software-application.csv\n","output_type":"stream"}],"execution_count":67}]}